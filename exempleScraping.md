# Exemple de l'analyse des sentiments a partir de facebook
## cas d'exemple  :
post de facebook sur coupe de monde 2030 : 
https://web.facebook.com/MustaphaSwinga/posts/pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l

# Installation et Requirements

## Anaconda Installation

Pour exÃ©cuter ce Jupyter Notebook, vous pouvez installer Anaconda, qui est une distribution Python puissante utilisÃ©e pour la science des donnÃ©es et l'apprentissage automatique. Vous pouvez tÃ©lÃ©charger Anaconda depuis [ce lien](https://www.anaconda.com/products/distribution) et suivre les instructions d'installation spÃ©cifiques Ã  votre systÃ¨me d'exploitation.
## Installation des Packages
Installez les packages nÃ©cessaires en utilisant pip depuis le terminal Powershell promp dans anaconda .
- pip install facebook_scraper
- pip install nltk 


```python
# Ã§a marche pas pour les pages jusqu'a maintenant
#from facebook_scraper import get_posts

#for post in get_posts('nintendo' ,pages=13):
#    print('test')
#    print(post['text'][:50])
```

    WARNING:facebook_scraper.page_iterators:No raw posts (<article> elements) were found in this page.
    


```python
"""
Download comments for a public Facebook post.
"""

import facebook_scraper as fs

# get POST_ID from the URL of the post which can have the following structure:
# https://www.facebook.com/USER/posts/POST_ID
# https://www.facebook.com/groups/GROUP_ID/posts/POST_ID
POST_ID = "pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l"

# number of comments to download -- set this to True to download all comments
MAX_COMMENTS = 500

# get the post (this gives a generator)
gen = fs.get_posts(
    post_urls=[POST_ID],
    options={"comments": MAX_COMMENTS, "progress": True}
)

# take 1st element of the generator which is the post we requested
post = next(gen)

# extract the comments part
comments = post['comments_full']

# process comments as you want...
for comment in comments:

    # e.g. ...print them
    print(comment['comment_text'])

    # e.g. ...get the replies for them
    for reply in comment['replies']:
        print(' ', reply['comment_text'])
```

    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Exception while running extract_user_id: KeyError('content_owner_id_new')
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_video didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_video_thumbnail didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_video_id didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_video_meta didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_factcheck didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_share_information didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_listing didn't return anything
    WARNING:facebook_scraper.extractors:[pfbid02bhaHCNCutE3S2p8S9Dsw7AeRTpDKEBE8qPENnZZChYSz14EBTi9kYgbArNuiwjn8l] Extract method extract_with didn't return anything
    


      0%|          | 0/500 [00:00<?, ?it/s]


    Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙÙ‚Ø· Ù‡ÙŠ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ³ØªÙØ§Ø¯ Ù…Ù†Ù‡Ø§ ØŒ Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ø³ØªÙƒÙˆÙ† Ø¥Ø³ØªÙØ§Ø¯Ø© Ù„Ø­Ø¸ÙŠØ© ÙÙ‚Ø· . Ø®ÙŠØ± Ù…Ø«Ø§Ù„ Ø¬Ø§Ø¦Ø­Ø© ÙƒÙˆØ±ÙˆÙ†Ø§ Ùˆ Ù„Ù… ÙŠØ³ØªÙØ¯ Ù…Ù†Ù‡Ø§ ...!!!!
      Abdelhamid Hajrifi ÙƒØ§ØªØ³Ù†Ø§ ØªØ±Ø¨Ø­ Ù…Ù† Ø¬Ø§Ø¦Ø­Ø© ØŸØŸØŸØŸ
      Abdelmonaime Chourafi Ø¯ÙˆÙ„ ÙƒØ«ÙŠØ±Ø© Ø¥Ø³ØªÙØ§Ø¯Øª Ù…Ù† Ø§Ù„Ø¬Ø§Ø¦Ø­Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø°ÙŠ Ù„Ù… Ù†Ø³ØªÙØ¯ Ù…Ù†Ù‡Ø§ Ø´ÙŠØ¦Ø§ ØŒ Ø£ØªØ­Ø¯Ø« Ø¹Ù† Ø§Ù„Ù…Ù†Ø¸ÙˆÙ…Ø© Ø§Ù„ØµØ­ÙŠØ© Ø§Ù„Ù…ØªÙ‡Ø§Ù„ÙƒØ© ØŒ Ø§Ù„Ø¨ÙŠØ±ÙˆÙ‚Ø±Ø§Ø·ÙŠØ© ØŒ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ùˆ ØºÙŠØ±Ù‡Ø§ ... Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ¹ØªØ¨Ø± Ø§Ù„Ø±Ø¨Ø­ Ø´ÙŠØ¦Ø§ Ø¢Ø®Ø± Ø§Ù„Ù„Ù‡ Ø£Ø¹Ù„Ù…
      Abdelhamid Hajrifi
    Ù‡Ù‡Ù‡Ù‡ Ø±Ø§Ùƒ Ù…Ø§ÙØ§Ù‡Ù… ÙˆØ§Ù„Ùˆ Ø£Ø®ÙˆÙŠØ§
    ØºØ§Ø­Ø§Ù„ ÙÙ…Ùƒ .. ÙˆØ§Ø´ ØªØªØ¹Ø±Ù Ø´Ø­Ø§Ù„ Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„ØµØ¹Ø¨Ø© ØºØ§Ø¯ÙŠ Ø¯Ø®Ù„Ù‡Ø§ Ù„ÙŠÙƒ ÙƒØ§Ø³ Ø§Ù„Ø¹Ø§Ù„Ù… Ø¨Ø³Ø¨Ø§Ø¨ Ø´Ù‡Ø±Ø© Ø¯ÙŠØ§Ù„ Ø¯ÙˆÙ„Ø© Ø¯ÙŠØ§Ù„Ùƒ ÙØ§Ù„Ø¹Ø§Ù„Ù… ØŸ Ø§Ù„Ø¹Ø§Ù„Ù… ÙƒØ§Ù…Ù„ ØºØ§Ø¯ÙŠ ÙŠØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…ØºØ±Ø¨ Ùˆ Ù…Ù„Ø§ÙŠÙŠÙ† ØºØ§Ø¯ÙŠ ÙŠØ±Ø¬Ø¹Ùˆ Ù„ÙŠÙ‡ Ø¨Ø¹Ø¯ Ø§Ù†ØªÙ‡Ø§Ø¡ ÙƒØ£Ø³ Ø§Ù„Ø¹Ø§Ù„Ù… ÙØ§Ù„Ø³Ù†ÙˆØ§Øª Ø§Ù„ÙŠ ØªÙ„ÙŠ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„
    ÙˆØ§Ø´ ØªÙŠØµØ­Ø§Ø¨Ù„Ùƒ Ø§Ù„Ø¯ÙˆÙ„ Ù…ÙƒÙ„Ø®Ø© Ø¨Ø§Ø´ ØªØ®Ø³Ø± Ù…Ù„Ø§ÙŠÙŠØ± Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±Ø§Øª ÙÙŠ ØªÙ†Ø¸ÙŠÙ… Ù‡Ø°Ù‡ Ø§Ù„ØªØ¸Ø§Ù‡Ø±Ø© Ù‡Ø§ÙƒØ§ ØºÙŠØ± ØµØ¯Ù‚Ø© Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
      Khalid El Haddalahi ØªØ¨Ø§Ø±Ùƒ Ø¹Ù„ÙŠÙƒ Ø§Ø®Ø§ÙŠ Ùˆ Ø§Ù„Ù„Ù‡ Ù…Ø§ ÙƒÙ†Øª Ø¹Ø§Ø±Ù Ù…Ø§ Ø³Ø­Ø§Ø¨Ù„ÙŠØ´ Ø§Ù†Øª Ø¹Ø§Ø±Ù Ù‡Ø§Ø¯Ø´ÙŠ ÙƒØ§Ù…Ù„ Ùˆ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ… Ø¹Ù†Ø¯Ùƒ Ø§Ù„ØµØ­ ... ÙÙŠÙ‚ Ø§Ø®Ø§ÙŠ Ø§Ù„Ù„Ù‡ ÙŠÙ‡Ø¯ÙŠÙƒ ...Ø±Ø§Ù‡ Ø¹Ø§Ø¯ ÙƒØ§Ù† Ø¹Ù†Ø¯Ù†Ø§ Ø§Ù„Ø²Ù„Ø²Ø§Ù„ ØŒ Ø§Ù„Ù‡ÙŠÙ„ÙŠÙƒÙˆØ¨ØªÙŠØ± Ù…Ù„Ù‚Ø§ØªØ´ Ù…Ù†ÙŠÙ† Ø¯ÙˆØ² ...
      Khalid El Haddalahi
    Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„ØµØ¹Ø¨Ø© Ù…ØºØªØ§Ø®Ø¯Ù‡Ø§Ø´ Ù†ØªØ§ ÙˆÙ…ØºØ§Ø¯ÙŠ ØªØ³ØªØ§ÙØ¯ Ù…Ù†Ù‡Ø§ ÙˆØ§Ù„Ùˆ. Ù†ØªØ§ ØºØ§Ø¯ÙŠ ØªØ§Ø®Ø¯ ØºÙŠØ± Ø§Ù„Ø²ÙŠØ§Ø¯Ø© ÙÙ„ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ø¨Ø±Ø§ÙƒØ© Ø¹Ù„ÙŠÙƒ.
      Abdelhamid Hajrifi
    Ù…Ø§ Ø¹Ù„Ø§Ù‚Ø© ÙØ³Ø§Ø¯ Ø§Ù„Ø¯ÙˆÙ„Ø©
    Ø¨Ø§Ù„Ø¥Ø³ØªÙØ§Ø¯Ø© Ù…Ù† ØªØ¸Ø§Ù‡Ø±Ø© ØŸ
    Ø­Ù†Ø§ ÙƒÙ†Ù‡Ø¶Ø±Ùˆ Ø¹Ù„Ù‰ ÙˆØ§Ø´ Ù‡Ø§Ø¯ Ø§Ù„ØªØ¸Ø§Ù‡Ø±Ø© Ù…ÙÙŠØ¯Ø© Ù„Ù„Ù…ØºØ±Ø¨ ÙˆØ§Ù†Øª ØªØªØ¬Ø§ÙˆØ¨ Ø¨Ø§Ù† Ø§Ù„Ù…ØºØ±Ø¨ ÙÙŠÙ‡ Ø§Ù„ÙØ³Ø§Ø¯ Ùˆ Ø§Ù„ÙÙ‚Ø± Ø§Ù„Ø®
    Ø´Ø­Ø§Ù„ Ø§Ù„IQ Ø¯ÙŠØ§Ù„Ùƒ Ø§Ø®ÙˆÙŠØ§ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
      Ø§Ù„Ø¨Ø·Ø§Ø·Ø³ Ø¨ 50 Ø¯Ø±Ù‡Ù…
      Soufiane Moutanabih
    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ Ù‡Ø§Ø¯ Ø§Ù„ÙÙŠØ³Ø¨ÙˆÙƒ ÙÙŠÙ‡ ØºØ§Ù„Ø¨Ø³Ø·Ø§Ø¡
    Ø³ÙŠØ± Ø£Ø®ÙˆÙŠØ§ ØªØ´Ø±ÙŠ Ø§Ù„Ø­Ø±Ø´Ø© Ùˆ ÙƒØ§Ø³ Ø¯ÙŠØ§Ù„Ùƒ Ø§Ù„Ù‚Ù‡ÙˆØ© ÙˆØªÙØ±Ø¬ ÙØ§Ù„ØªØ´Ø§Ù…Ø¨ÙŠÙˆÙ†Ø³ Ù‡Ø§Ø¯Ø´ÙŠ Ø¨Ø¹ÙŠØ¯ Ø¹Ù„ÙŠÙƒ
    ÙˆØ§Ø´ ØªØªØ³Ù†Ø§ ÙŠØ¯Ù‚Ùˆ Ø¹Ù„ÙŠÙƒ ÙØ§Ù„Ø¯Ø§Ø± Ùˆ ÙŠØ¹Ø·ÙˆÙƒ Ø¬ÙˆØ§ ÙÙŠÙ‡ Ø§Ù„ÙÙ„ÙˆØ³ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
    Ø¯ÙŠØ± Ø´ÙŠ Ø¨Ø­Ø« ÙØ¬ÙˆØ¬Ù„ ÙˆÙˆØ¹Ø±Ù ÙƒÙØ§Ø´ Ù„Ù„Ø³ÙŠØ§Ø­Ø© Ø£Ù†Ù‡Ø§ ØªØ±ÙˆØ¬ Ù„ÙŠÙƒ Ø§Ù„Ø¯ÙˆÙ„Ø© .. Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù‚Ù„ ØªÙÙ‡Ù… Ø´Ù†Ùˆ Ø·Ø§Ø±ÙŠ
      Khalid El Haddalahi Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
    Ù‡Ù‡Ù‡
      Khalid El Haddalahi Ø¨ØµØ­ØªÙƒ Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„ØµØ¹Ø¨Ø© ÙˆØ§Ù„Ù…Ù„Ø§ÙŠÙŠØ± Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¯ÙˆÙ„Ø§Ø±.
    Ø£Ø¬ÙŠ Ø¹Ù†Ø¯Ùƒ Ø¨Ø¹Ø¯Ø§ ÙÙŠÙ† ØªØ®Ø¨ÙŠ Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„ØµØ¹Ø¨Ø© Ø£ÙˆÙ„Ø§ Ù„Ø§ØŸ
      Abdelhamid Hajrifi ÙŠØ¹Ù†ÙŠ ÙÙŠ Ù†Ø¸Ø±Ùƒ ÙƒÙˆØ±ÙˆÙ†Ø§ ÙƒØ§Ù†Øª Ø®Ø§ØµÙ‡Ø§ ØªØ¬Ø¹Ù„Ù†Ø§ Ù†ØªØ·ÙˆØ±Ùˆ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
    Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡
      Khalid El Haddalahi
    Ù‡Ø°Ø§ ØºÙŠØ± ØªÙÙƒÙŠØ± Ø§Ù„Ø¹Ø¨ÙŠ.Ø¯ ÙˆØ§Ù„Ù…Ø°Ù„ÙˆÙ„ÙŠÙ† Ø¨Ø­Ø§Ù„Ùƒ Ù„Ø§Ø­Ø§Ø³ÙŠÙ† Ø§Ù„Ø¹Ø±Ø´ ÙˆØµØ­Ø§Ø¨ Ø§Ù„Ø²Ù„ÙŠØ¬ Ø¯ÙŠØ§Ù„Ù†Ø§ ÙˆØ§Ù„Ù‚Ø·ÙØ§Ù† Ø¯ÙŠØ§Ù„Ù†Ø§ ÙˆÙŠØ§Ø§Ø§Ø§Ø§Ø§Ø§ Ø±Ø¨Ø­Ù†Ø§. ØªØ§ ÙˆØ§Ø­Ø¯ Ù…ÙƒÙŠØªØ³Ù†Ù‰ Ø§Ù„Ø¯ÙˆÙ„Ø© ØªØ¹Ø·ÙŠÙ‡ Ø§Ù„ÙÙ„ÙˆØ³ ÙØ¯Ø§Ø±. ÙˆÙ„ÙƒÙ† Ø­Ù†Ø§ Ø¨ØºÙŠÙ†Ø§ Ø¹ÙŠØ± ØªØ¹Ù„ÙŠÙ… Ù…Ø²ÙŠØ§Ù† ÙˆØµØ­Ø© Ù…Ø²ÙŠØ§Ù†Ø© ÙˆØ¨Ù†ÙŠØ© ØªØ­ØªÙŠØ© Ù…Ø²ÙŠØ§Ù†Ø©. Ù‡Ø§Ø¯ÙŠ Ø£Ù…ÙˆØ± Ø¨Ø³ÙŠØ·Ø© Ø¨ØºÙŠÙ†Ø§Ù‡ Ø§ ÙˆØ³Ø§Ù‡Ù„ Ø¬Ø¯Ø§Ù‹ Ø¨Ø§Ø´ Ù†Ø­Ù‚Ù‚ÙˆÙ‡Ø§ ÙˆÙ„ÙƒÙ† Ù„ÙŠ Ù…Ø³Ø¤ÙˆÙ„ÙŠÙ† Ø¹Ù„Ù‰ Ù‡Ø§Ø¯ Ø§Ù„Ø¨Ù„Ø§Ø¯ ÙƒÙŠØ¶Ø±ÙŠÙˆ ÙƒÙ„Ø´ÙŠ Ù„Ø¬ÙŠØ¨Ù‡Ù… ÙˆØ´ÙƒÙˆÙ† ØªÙŠØ®Ù„ØµØŸ ØªÙŠØ®Ù„Øµ Ø§Ù„Ù…ÙˆØ§Ø·Ù† Ø§Ù„Ø¶Ø¹ÙŠÙ Ø§Ù„Ù…Ù‚Ù‡ÙˆØ±ØŒ Ø¹Ø±ÙØªÙŠ Ø¹Ù„Ø§Ø´ØŸ Ø­ÙŠØª ÙƒØ§ÙŠÙ†ÙŠÙ† Ø§Ù„Ø¹Ø¨ÙŠ.Ø¯ Ø¨Ø­Ø§Ù„Ùƒ Ù„ÙŠ ÙƒÙŠØ·Ø¨Ù„Ùˆ Ù„Ù„ØªØ®Ø±Ø¨ÙŠÙ‚ ÙˆÙƒÙŠÙ‚ÙˆÙ„ÙˆØ´ Ø§Ù„Ø­Ù‚. Ú¯Ø·Ø¹ Ø§Ù„Ù„Ù‡ ÙŠØ¯ÙŠØ± Ø®ÙŠØ± ÙˆØ®Ø§ Ù…Ø§ Ø¨Ø§Ù† Ù„ÙŠØ§ Ø®ÙŠØ± Ù…Ø¹ Ø§Ù„Ù…Ø°Ù„ÙˆÙ„ÙŠÙ† Ù„ÙŠ Ù‡Ù…Ù‡Ù… ØºÙŠØ± Ø§Ù„ÙƒØ±Ø© ÙˆØ§Ù„Ø¹Ù‡Ø±.
   .................
    

## passage vers pandas (bibliotheque pour manipulation des dataframes )


```python
#importation de package pandas as pd
import pandas as pd
df = pd.json_normalize(comments, sep='_')
df.shape

```




    (510, 13)




```python
#extrait de donnes
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>comment_id</th>
      <th>comment_url</th>
      <th>commenter_id</th>
      <th>commenter_url</th>
      <th>commenter_name</th>
      <th>commenter_meta</th>
      <th>comment_text</th>
      <th>comment_time</th>
      <th>comment_image</th>
      <th>comment_reactors</th>
      <th>comment_reactions</th>
      <th>comment_reaction_count</th>
      <th>replies</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>163616990078043</td>
      <td>https://facebook.com/163616990078043</td>
      <td>100000434229818</td>
      <td>None</td>
      <td>Abdelhamid Hajrifi</td>
      <td>None</td>
      <td>Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙÙ‚Ø· Ù‡ÙŠ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ³ØªÙØ§Ø¯ Ù…Ù†Ù‡Ø§...</td>
      <td>2023-10-12</td>
      <td>None</td>
      <td>[]</td>
      <td>None</td>
      <td>None</td>
      <td>[{'comment_id': '245859991376592', 'comment_ur...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>336649735558185</td>
      <td>https://facebook.com/336649735558185</td>
      <td>100004767259454</td>
      <td>None</td>
      <td>Abderrahim Baalla</td>
      <td>None</td>
      <td>ÙØ±ØµØ© Ø§Ù‚ØªØµØ§Ø¯ÙŠØ© Ù…Ø²ÙŠØ§Ù†Ø© Ø¨Ø§Ø´ ØªØ¬Ø°Ø¨ Ø§Ù„Ø§Ø³ØªØ«Ù…Ø§Ø±Ø§Øª ØŒ Ùˆ ...</td>
      <td>2023-10-12</td>
      <td>None</td>
      <td>[]</td>
      <td>None</td>
      <td>None</td>
      <td>[{'comment_id': '774438894484417', 'comment_ur...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>333864169044501</td>
      <td>https://facebook.com/333864169044501</td>
      <td>100001654654191</td>
      <td>None</td>
      <td>Hanane Salma</td>
      <td>None</td>
      <td>ØªØ­ÙŠØ© Ù„Ù„Ø£Ø³Ø§ØªØ°Ø© âœŒğŸ»\n#Ù…Ø¹Ø§_Ù„Ø±Ø¯_Ø§Ù„Ø§Ø¹ØªØ¨Ø§\nØ±_Ù„Ù„Ø£Ø³ØªØ§Ø°_...</td>
      <td>2023-10-12</td>
      <td>None</td>
      <td>[]</td>
      <td>None</td>
      <td>None</td>
      <td>[{'comment_id': '344416661489702', 'comment_ur...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1479996756173048</td>
      <td>https://facebook.com/1479996756173048</td>
      <td>100001866400174</td>
      <td>None</td>
      <td>Alae El Alami</td>
      <td>None</td>
      <td>Ø£Ø­Ø³Ù† Ø­Ø§Ø¬Ø© Ù‡ÙŠ ÙŠØ®Ø¯Ù…Ùˆ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ø¯ÙŠØ§Ù„ Ø§Ù„Ø¨Ù„Ø§Ø¯</td>
      <td>2023-10-12</td>
      <td>None</td>
      <td>[]</td>
      <td>None</td>
      <td>None</td>
      <td>[]</td>
    </tr>
    <tr>
      <th>4</th>
      <td>276852758637432</td>
      <td>https://facebook.com/276852758637432</td>
      <td>100000857307632</td>
      <td>None</td>
      <td>Ø±Ø´ÙŠØ¯ Ø§Ù„Ù…Ù†ØµÙˆØ±ÙŠ</td>
      <td>None</td>
      <td>Ù„Ø§ Ø´ÙŠØ¡ Ù„Ù… ÙŠØ¹Ø¯ Ø§Ù„Ù…ÙˆÙ†Ø¯ÙŠØ§Ù„ Ø§Ù„ØªØ§Ø«ÙŠØ± Ø§Ù„ÙƒØ¨ÙŠØ± Ø¹Ù„Ù‰ Ø§Ù„Ø§...</td>
      <td>2023-10-12</td>
      <td>None</td>
      <td>[]</td>
      <td>None</td>
      <td>None</td>
      <td>[]</td>
    </tr>
  </tbody>
</table>
</div>




```python
# Sauvegarder le DataFrame dans un fichier CSV
df.to_csv('commentaires.csv', index=False)

```

# petite exemple d'analyse de text avec nltk (analyse basic)
NLTK (Natural Language Toolkit) est une bibliothÃ¨que Python puissante pour le traitement du langage naturel. Elle offre de nombreux outils et ressources pour analyser et traiter le texte.




```python
import nltk
nltk.download('punkt')
```

    [nltk_data] Downloading package punkt to
    [nltk_data]     C:\Users\sejja\AppData\Roaming\nltk_data...
    [nltk_data]   Unzipping tokenizers\punkt.zip.
    




    True




```python
nltk.download('stopwords')
```

    [nltk_data] Downloading package stopwords to
    [nltk_data]     C:\Users\sejja\AppData\Roaming\nltk_data...
    [nltk_data]   Unzipping corpora\stopwords.zip.
    




    True




```python
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from collections import Counter
import matplotlib.pyplot as plt

# Assurez-vous d'avoir nltk tÃ©lÃ©chargÃ©
# nltk.download('punkt')
# nltk.download('stopwords')

# Supprimez les commentaires nuls ou vides
df = df.dropna(subset=['comment_text'])
df = df[df['comment_text'] != '']

# ConcatÃ©nez tous les commentaires en une seule chaÃ®ne
all_comments = ' '.join(df['comment_text'])

# Tokenization
tokens = word_tokenize(all_comments)

# Supprimez la ponctuation et convertissez en minuscules
tokens = [word.lower() for word in tokens if word.isalpha()]

# Supprimez les stopwords
stop_words = set(stopwords.words('arabic'))  # AdaptÃ© pour l'arabe
tokens = [word for word in tokens if word not in stop_words]

# Comptez les occurrences de chaque mot
word_counts = Counter(tokens)

# Affichez les 10 mots les plus frÃ©quents
most_common_words = word_counts.most_common(10)
print("Les 10 mots les plus frÃ©quents :")
for word, count in most_common_words:
    print(f"{word}: {count}")

# CrÃ©ez un graphique des mots les plus frÃ©quents
plt.bar(*zip(*most_common_words))
plt.xlabel('Mots')
plt.ylabel('FrÃ©quence')
plt.title('Top 10 des mots les plus frÃ©quents')
plt.show()

```

    Les 10 mots les plus frÃ©quents :
    ØºØ§Ø¯ÙŠ: 77
    Ø§Ù„ØªØ­ØªÙŠØ©: 59
    Ø§Ù„Ù…ØºØ±Ø¨: 59
    Ø§Ù„Ø¨Ù†ÙŠØ©: 51
    Ø§Ù„Ù„Ù‡: 48
    Ø§Ù„Ø¹Ø§Ù„Ù…: 47
    Ø´ÙŠ: 35
    Ø¨Ø§Ø´: 33
    Ø§Ù„Ø´Ø¹Ø¨: 32
    ØªÙ†Ø¸ÙŠÙ…: 32
    


    
![png](output_11_1.png)
    

